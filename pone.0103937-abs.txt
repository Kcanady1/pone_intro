Introduction

1 Reliability Design

1.1 Reliability vs. Quality
Quality and Reliability are two major concerns in the engineering industry, especially for manufacturing and civil engineering. Quality relates to a product or structure's initial performance when new, while reliability relates to performance over its lifetime. Over the years many industrial standards and evaluation systems have been built relating to quality, such as Six Sigma, ISO 9000, the Malcolm Baldrige National Quality Award and the Deming Prize. The American Society for Quality (ASQ) has garnered well over 150,000 members and continues to grow in strength. By comparison reliability is fairly neglected; the Institute for Reliability Engineering languishes is near obscurity. However, for the end customer reliability is equally, if not more important than quality because of future maintenance costs and the consequences of potential failures in today's large, complex systems.
In this paper we focus on reliability. We propose new and more systematic methodologies to discover destructive energies and apply the methods to the pipe in a subsea pipeline for demonstration purposes. In addition to incorporating prior experience in a design to avoid known failure mechanisms, reliability can be improved by exploring for unknown weak links and interactions resulting from the multiple, simultaneous stresses that a pipeline may experience during its operational life. The pipe should survive the compound impact from both external environment and internal stresses throughout its designed lifetime, say, for 20 years or more.

1.2 Monumental economic loss of due to lack reliability
Keki Bhote in his book, World Class Reliability states, “Many companies take a cavalier attitude in dismissing their costs associated with reliability. They express their satisfaction if warranty costs are within a few percent of their sales dollars. They may justify such costs by including them in their budgets. Tragically, most companies do not recognize that their own warranty cost is only the tip of the iceberg” [1]. The total cost of reliability failures includes not only direct repair costs but also the costs of supplier retrofits, lawsuits, and third party costs to their customer's customers, to society, to governments and environmental damage. These costs are virtually never included in an economic analysis.
James R. Chiles in his book, Inviting Disaster; Lessons from the Edge of Technology, cites numerous examples where simply design changes could have avoided many high visibility catastrophic failures [2]. To wit, the 1979 partial meltdown of a Three Mile Island nuclear power plant reactor, the 1980 capsizing of the Alexander Keilland floating platform to house offshore oil workers, the 1982 sinking of the Ocean Ranger offshore floating drill rig, the 1986 explosion of the Chernobyl nuclear power station, the 1986 mid-air breakup of the Space Shuttle Challenger, the 1988 explosion and fire on the Piper Alpha offshore drilling rig, the 1999 loss of the Mars Polar Lander, the 2000 crash of the Air France Concord departing Charles de Gaulle Airport, and many others. Since the publication of his book, the litany of such disasters continues with such high profile events as the 2010 Deepwater Horizon oil spill in the Gulf of Mexico and the 2011 meltdown of the Fukushima nuclear power plant following an earthquake. Many of these disasters could have been avoided with adequate stress testing to find the weak links and designing them out, or in some cases, simply not ignoring known weak links.

2 Weaknesses of current practices regarding reliability

2.1 Logic-based approaches for risk factor identification
Many logic-based approaches have been applied to reliability risk analysis. Fault Tree Analysis, FTA, has been used for identifying and ranking risk factors based on their assigned probabilities [3]. Bayesian Belief Networks build a network used to demonstrate the logic relationship between factors through a probability distribution calculation [4]. Failure Mode and Effects Analysis, FMEA, is used to evaluate the effect of potential failures of designed functions [5]. Cause and defect Diagrams, also called Fishbone Diagrams, are used to identify potential factors which would cause the system to fail [6], etc. All these methods are based on knowledge and experience; factors are often identified through brainstorming. They are useful early in the design phase to prevent incorporating failure modes which have previously been discovered or which are suspected. However, their weakness lies in the fact that unknown factors and interactions are missed – the very factors that are often responsible for so-called “rare events” or “Black Swans” [7].

2.2 Mathematical modeling
Reliability testing often includes mathematical modeling. These models are typically expressed graphically or with mathematical functions. Researchers design experiments to simulate the relationship between the input parameters/factors and the major output functions. In order to increase fidelity of the model, complexity is incrementally added following analysis and inference, or from other data. But it is impossible to predict with certainty whether a given mathematical model is adequate or not [8].
Other fundamental weaknesses of the mathematical approach are: 1) Large sample sizes required to test the model. For example, based on the binomial proportion confidence interval, doubling the number of failure modes will triple the sample size [9]. 2) Much time is consumed for direct testing to a significant portion of life. 3) High confidence levels are difficult to achieve (50% is meaningless, 70% is not a whole lot better, 90–95% is desirable) because the quantities, costs and test times become astronomical as confidence level requirements increase.
Having noted the deficiencies of mathematical modeling, we hasten to point out that such models are invaluable for functional design before life testing begins. In this situation designers are trying to select and apply the physical laws of nature necessary to make the system function as intended. However, in the case of reliability it is more important for designers to prevent unexpected behavior of the system. This is a much broader challenge and requires a different experimental strategy, such as accelerated life testing.

2.3 Accelerated lifetime tests
Accelerated life testing (ALT) attempts to bring out unexpected weak links and interactions in a design. These are failure modes that were not or could not be predicted by prior modeling, design and prototyping. Various ALT methods are in use, such as HALT (Highly Accelerated Life Testing) and HASS (High Accelerated Stress Screen) [10] which apply two basic stresses, temperature and vibration typically, in parallel or sequentially, along with other special stresses associated with the system's operation. Another example is HAET (High Accelerated Environmental Testing), in which a stimulation mechanism is introduced in order to quantify the factors/failure modes at “end of life”.
However, there are certain shortcomings of most ALT methods, which may include: 1) Environmental stresses are often assumed to be single-factor effects, not combined and not interacting. 2) Over-stress levels could be insufficient to detect the most important failure modes. 3) Incremental stress step size could be too small to yield results within a suitable time period or too large and skip over important failure modes. 4) Excessive stresses may introduce artificial failures which would never occur in the field. 5) A large number of test units are needed for each failure mode. 6) Converting time-to-failure to a calculated lifetime is risky because overstressing is likely to behave in an unpredictable, non-linear way, depending on the overstress levels applied.
Our preferred approach is to use Multiple Environment Over Stress Testing (MEOST) primarily because it avoids shortcomings 1–5, and does not attempt to predict lifetime as #6 above. Rather it focuses on assuring that full life can be reached at full stress without experiencing unexpected failure modes from weak links and interactions. The method is briefly described below, and we develop new methods to assure more systematic discovery of the energies that should be fed into an MEOST experiment.

2.4 Function Models in reliability
Function models are often used to explain in graphical form how a system is supposed to work in terms of how its subsystems link and operate together to achieve the overall goal(s). Commonly used function models are: Function Tree, which builds the function structure by decomposing the major function(s) into individual sub-functions based on the logic relationship between them [11]; Function Analysis System Technique (FAST), a similar method introduced by the Society of American Value Engineer in 1965 [12]; Structured Analysis and Design Technique (SADT) employs a diagrammatic notation designed specifically to help people describe and understand systems [13].
Such models are excellent from the perspective of developing the required system functionality and, given good design, components and assembly, the initial system quality should be good. However, from a reliability perspective there is an important aspect which needs to be added: energy or energy combinations which cause failure. In particular we are concerned with (a) external/environmental energies, (b) energies employed in the system which lead to stresses on the system, and (c) energy losses which occur as energy is transferred among system sub-functions. The second law of thermodynamics assures us that some energy will be lost during transformation; part of that lost energy may go into destructive behavior.
